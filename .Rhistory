library(segsimflex)
install.packages("path/to/newPackage_0.1.0.tar.gz",
repos=NULL, type="source")
usethis::use_mit_license()
devtools::document()
devtools::document()
install.packages('keras')
library(tensorflow)
tf$constant('Hello world')
Y
tf$constant('Hello world')
library(keras)
library(keras)
install_keras()
library(keras)
install_keras()
install.packages('keras')
install.packages("keras")
library(keras)
library(tensorflow)
tf$constant('Hello world')
install_keras()
library(keras)
install_keras()
remove.packages("keras")
install.packages('keras')
install.packages('keras')
library(keras)
install_keras()
n
library(tensorflow)
tf$constant('Hello world')
# packages
library(tidyverse)
library(tidymodels)
library(tidytext)
library(keras)
library(tensorflow)
# data location
url <- 'https://raw.githubusercontent.com/pstat197/pstat197a/main/materials/labs/lab6-nn/data/claims-clean.csv'
# read in data
clean <- read_csv(url)
# partition
set.seed(102722)
partitions <- clean %>%
mutate(text_clean = str_trim(text_clean)) %>%
filter(str_length(text_clean) > 5) %>%
initial_split(prop = 0.8)
# partition
set.seed(102722)
partitions <- clean %>%
mutate(text_clean = str_trim(text_clean)) %>%
filter(str_length(text_clean) > 5) %>%
initial_split(prop = 0.8)
train_dtm <- training(partitions) %>%
unnest_tokens(output = 'token',
input = text_clean) %>%
group_by(.id, bclass) %>%
count(token) %>%
bind_tf_idf(term = token,
document = .id,
n = n) %>%
pivot_wider(id_cols = c(.id, bclass),
names_from = token,
values_from = tf_idf,
values_fill = 0) %>%
ungroup()
# extract first ten features
x_train <- train_dtm %>%
ungroup() %>%
select(-.id, -bclass) %>%
select(1:10) %>%
as.matrix()
# extract labels and coerce to binary
y_train <- train_dtm %>%
pull(bclass) %>%
factor() %>%
as.numeric() - 1
# specify model type
model <- keras_model_sequential(input_shape = 10)
summary(model)
# add output layer
model <- model %>% layer_dense(1)
summary(model)
model <- model %>%
layer_activation(activation = 'sigmoid')
model %>% compile(
loss = 'binary_crossentropy',
optimizer = optimizer_sgd(),
metrics = 'binary_accuracy'
)
history <- model %>%
fit(x = x_train,
y = y_train,
epochs = 10)
# retrieve weights
get_weights(model)
history <- model %>%
fit(x = x_train,
y = y_train,
epochs = 10)
# retrieve weights
get_weights(model)
# evaluate on specified data
evaluate(model, x_train, y_train)
# compute predictions
model(x_train) %>% head()
# store full DTM as a matrix
x_train <- train_dtm %>%
select(-bclass, -.id) %>%
as.matrix()
summary(model)
model %>%
compile(
loss = 'binary_crossentropy',
optimizer = optimizer_sgd(),
metrics = 'binary_accuracy'
)
plot(history)
# change the optimizer
model %>%
compile(
loss = 'binary_crossentropy',
optimizer = 'adam',
metrics = 'binary_accuracy'
)
# re-train
history <- model %>%
fit(x = x_train,
y = y_train,
epochs = 10)
# evaluate on specified data
evaluate(model, x_train, y_train)
# add output layer
model <- model %>% layer_dense(1)
# specify model type
model <- keras_model_sequential(input_shape = 10)
summary(model)
# add output layer
model <- model %>% layer_dense(1)
summary(model)
model <- model %>%
layer_activation(activation = 'sigmoid')
model %>% compile(
loss = 'binary_crossentropy',
optimizer = optimizer_sgd(),
metrics = 'binary_accuracy'
)
history <- model %>%
fit(x = x_train,
y = y_train,
epochs = 10)
# retrieve weights
get_weights(model)
# packages
library(tidyverse)
library(tidymodels)
library(tidytext)
library(keras)
library(tensorflow)
# data location
url <- 'https://raw.githubusercontent.com/pstat197/pstat197a/main/materials/labs/lab6-nn/data/claims-clean.csv'
# read in data
clean <- read_csv(url)
# partition
set.seed(102722)
partitions <- clean %>%
mutate(text_clean = str_trim(text_clean)) %>%
filter(str_length(text_clean) > 5) %>%
initial_split(prop = 0.8)
train_dtm <- training(partitions) %>%
unnest_tokens(output = 'token',
input = text_clean) %>%
group_by(.id, bclass) %>%
count(token) %>%
bind_tf_idf(term = token,
document = .id,
n = n) %>%
pivot_wider(id_cols = c(.id, bclass),
names_from = token,
values_from = tf_idf,
values_fill = 0) %>%
ungroup()
# extract first ten features
x_train <- train_dtm %>%
ungroup() %>%
select(-.id, -bclass) %>%
select(1:10) %>%
as.matrix()
# extract labels and coerce to binary
y_train <- train_dtm %>%
pull(bclass) %>%
factor() %>%
as.numeric() - 1
# specify model type
model <- keras_model_sequential(input_shape = 10)
summary(model)
# add output layer
model <- model %>% layer_dense(1)
summary(model)
model <- model %>%
layer_activation(activation = 'sigmoid')
model %>% compile(
loss = 'binary_crossentropy',
optimizer = optimizer_sgd(),
metrics = 'binary_accuracy'
)
history <- model %>%
fit(x = x_train,
y = y_train,
epochs = 10)
# retrieve weights
get_weights(model)
# evaluate on specified data
evaluate(model, x_train, y_train)
# compute predictions
model(x_train) %>% head()
# store full DTM as a matrix
x_train <- train_dtm %>%
select(-bclass, -.id) %>%
as.matrix()
model <- keras_model_sequential(input_shape = ncol(x_train)) %>%
layer_dense(10) %>%
layer_dense(1) %>%
layer_activation(activation = 'sigmoid')
summary(model)
model %>%
compile(
loss = 'binary_crossentropy',
optimizer = optimizer_sgd(),
metrics = 'binary_accuracy'
)
history <- model %>%
fit(x = x_train,
y = y_train,
epochs = 50)
plot(history)
# change the optimizer
model %>%
compile(
loss = 'binary_crossentropy',
optimizer = 'adam',
metrics = 'binary_accuracy'
)
# re-train
history <- model %>%
fit(x = x_train,
y = y_train,
epochs = 10)
plot(history)
# redefine model
model <- keras_model_sequential(input_shape = ncol(x_train)) %>%
layer_dense(10) %>%
layer_dense(1) %>%
layer_activation(activation = 'sigmoid')
model %>%
compile(
loss = 'binary_crossentropy',
optimizer = 'adam',
metrics = 'binary_accuracy'
)
# train with validation split
history <- model %>%
fit(x = x_train,
y = y_train,
epochs = 20,
validation_split = 0.2)
plot(history)
devtools::build()
devtools::document()
devtools::document()
devtools::document()
devtools::document()
t1_first_nonsp = str_locate(t1, "[^ ]")[1]
record=data.frame()
e1="1"
e2="2"
new_row = c("s", e1, e2)
new_row
record[nrow(record) + 1,] = new_row
record
record = data.frame(type = NA, e1 =NA, e2= NA)
record[nrow(record) + 1,] = new_row
record
record[-1]
record[,-1]
record[-1,]
record = data.frame
record = data.frame()
rbind(record, new_row)
record=data.frame(type, e1, e2)
record=data.frame("type", "e1", "e2")
record
record = data.frame()
colnames(record)
colnames(record) = c("type", "e1", "e2")
record = data.frame()
rbind(record, new_row)
colnames(record) = c("type", "e1", "e2")
record =rbind(record, new_row)
colnames(record) = c("type", "e1", "e2")
record
currBlist1 = ", ,.?, ."
currBlist1 = ", ,.?, ."
currBlistList1 = sapply(1:nchar(currBlist1), function(x) substring(currBlist1, x, x))
currBlistList2 = sapply(1:nchar(currBlist2), function(x) substring(currBlist2, x, x))
currBlist2 = ", ,.?, ."
currBlistList2 = sapply(1:nchar(currBlist2), function(x) substring(currBlist2, x, x))
currBlistList2 = sapply(1:nchar(currBlist2), function(x) substring(currBlist2, x, x))
currBlistList2
sapply(1:nchar(currBlist1), function(x) substring(currBlist1, x, x) != " " & substring(currBlist2, x, x) != " ")
posMatch = sapply(1:nchar(currBlist1), function(x) substring(currBlist1, x, x) != " " & substring(currBlist2, x, x) != " ")
substPos = posMatch & sapply(1:nchar(currBlist1), function(x) substring(currBlist1, x, x) != substring(currBlist2, x, x))
substPos = posMatch & sapply(1:nchar(currBlist1), function(x) substring(currBlist1, x, x) != substring(currBlist2, x, x))
substPos
posMatch
currBlist1
currBlist2
currBlist2 = ",!!.., ,"
currBlistList2 = sapply(1:nchar(currBlist2), function(x) substring(currBlist2, x, x))
posMatch = sapply(1:nchar(currBlist1), function(x) substring(currBlist1, x, x) != " " & substring(currBlist2, x, x) != " ")
substPos = posMatch & sapply(1:nchar(currBlist1), function(x) substring(currBlist1, x, x) != substring(currBlist2, x, x))
substPos
which(substPos)
diag(7)
a=c(1,2,3,3,5,6)
a[1:2
]
c(a[1:3],a[2:4
])
